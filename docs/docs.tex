\documentclass{article}

\usepackage[margin=1in]{geometry}

\title{Program 1 - Perceptron}
\author{Logan Grosz}
\date{Last updated: \today}

\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}

\begin{document}

\maketitle

\section{Usage}

\subsection{Installing the module from source}

In the directory with \texttt{setup.py}

\texttt{pip install .}

\subsection{Usage}

Then anywhere...

\texttt{import <classifier> from myml}

\texttt{import myml.util as util}

For examples see \texttt{examples/}

\section{API Definitions}

\subsection{myml.util}

\texttt{plot\_decision\_regions(X, y, classifier, resolution)} :: Will print a 2D scattery plot along with a
line that separates two regions in accordance with the perceptron passed in.

\texttt{plot\_decision\_regions\_3d(X, y, classifier, resolution)} :: Will print a 3D scattery plot along with a
plane that separates two regions in accordance with the perceptron passed in.

\texttt{plot\_linear\_regression} :: Will print a 2D scattery plot along with a
plane that separates two regions in accordance with the perceptron passed in. For any higher dimensions it will not print. (For final portfolio, this will do a 3D plot as well)

\subsection{myml.Perceptron}

\subsubsection{Description}

The perceptron follows the data fitting algorithm presented on Wikipedia
closely. It initializes weights to 0. For each sample, it calculates the actual
output,and then changes the weight to reflect the difference between this output
and the desired output. This process repeats until the specified number of
iterations is reached or there is an iteration error rate of 0. In order to
preserve data, both the number of errors and weight at each step is recorded.
These can be accessed through \textit{Perceptron}\texttt{.weights} and
\textit{Perceptron}\texttt{.errors}.

\subsubsection{Usage}

\begin{verbatim}
pn = Perceptron(rate, epochs)
pn.fit(X, y)
pn.predict(Z)
\end{verbatim}

\texttt{X} is a numpy.ndarray of shape ($samples$, $features$)

\texttt{y} is a numpy.ndarray of shape ($samples$)

\subsubsection{Attributes}

\textit{Perceptron}\texttt{.rate} :: The learning rate of the classifier

\textit{Perceptron}\texttt{.epochs} :: The number of epochs to run when fitting. These are not guaranteed to run if the classifier can be fit with no error at any point.

\textit{Perceptron}\texttt{.errors} :: Errors at each epoch during fitting. This is a numpy.ndarray with shape ($epochs$, $features$).

\textit{Perceptron}\texttt{.weights} :: Weights at each epoch during fitting. This is a numpy.ndarray with shape ($epochs$, $features$).

\subsubsection{Methods}

\textit{Perceptron}\texttt{.\_\_init\_\_} :: The initilizer takes 2 arguments: the
first is the learning rate, a float between 0 and 1. The second is the max
number of iterations. These values can be accessed again with
\textit{Perceptron}\texttt{.rate} and \textit{Perceptron}\texttt{.niter}.

\textit{Perceptron}\texttt{.fit} :: Takes arguments \texttt{X} (numpy array
with shape (\textit{nSamples}, \textit{nFeatures})) and \texttt{d} (also a
numpy array with shape (\textit{nSamples})). This function applies the steps
described above and ``validates" the errors and weights arrays.

\textit{Perceptron}\texttt{.net\_input} :: Returns the weighted values of some
given input \texttt{X} (of type numpy array with shape
(\textit{nSamples, nFeatures})) as a numpy array with shape (\textit{nSamples}).

\textit{Perceptron}\texttt{.predict} :: Returns labels (-1 or 1) for a given
\texttt{X} of type numpy array with shape (\textit{nSamples, nFeatures}) as a
numpy array of shape (\textit{nSamples}).

\texttt{Perceptron.f} :: a static method which returns the label given
\texttt{w} (a numpy array of shape (\textit{nFeatures}+1)), and \texttt{x}, a
sample with shape (\textit{nFeatures}). Note, they're not the same size but
that's because \texttt{w[0]} acts as a bias.

\subsection{myml.LinearRegressor}

\subsubsection{Description}

The linear regressor fits data assumed to be linear in nature. It uses a gradient
descent algorithm on the squared error between expected and actual output values. This
can be done with any number of independant variables. It starts assuming the weights, $\beta = [0, ..., 0]$.
It then calulates partial derivatives of the error term $J$, for each independant variable in $X$.
$J$, the cost, is defined as the sum of the squared error for each sample point: $\sum_{i=1}^{n}(y_i-(\mathbf{\beta}\cdot\mathbf{X^T_i}))^2$.
Using the partial derivatives and the error rate, a new $\beta$ is determined for
the next epoch. This is repeated until the given number of epochs.

\subsubsection{Usage}

\begin{verbatim}
lr = LinearRegressor(rate, epochs)
lr.fit(X, y)
lr.predict(Z)
\end{verbatim}

\texttt{X} is a numpy.ndarray of shape ($n$, $p$)

\texttt{y} is a numpy.ndarray of shape ($n$)

where $n$ is the number of samples and $p$ is the number of independant variables

\subsubsection{Properties}

Note: Attributes are meant to be read-only. Modify them at your own risk. I have not chosen to enforce this since anyone can break it if they wanted anyways.

\textit{LinearRegressor}\texttt{.rate} :: learning rate

\textit{LinearRegressor}\texttt{.epochs} :: epochs

\textit{LinearRegressor}\texttt{.w} :: matrix of values of $\beta$. This is a numpy.ndarray of shape ($epochs$, $p$).

\textit{LinearRegressor}\texttt{.errors} :: matrix of values of $\epsilon$. $\epsilon$ is the error between each predicted $y_i$ and its real value, $d_i$, such that $\epsilon + y_i = d_i$

\subsubsection{Methods}

\textit{LinearRegressor}\texttt{.\_\_init\_\_(rate, epochs)} :: Simply sets the learning rate and attributes of a linear regressor so it's ready to be fitted.

\textit{LinearRegressor}\texttt{.fit(X, y)} :: Takes $X$ (numpy.ndarray of shape ($n$, $p$)) and $y$ (numpy.ndarray of shape ($n$)). $n$ is the number of samples while $p$ is the number of independant variables.

\textit{LinearRegressor}\texttt{.predict(X)} :: takes a matrix of $\mathbf{x}$s, and spits out their predicted $\textbf{y}$s using the $\beta$ of the last epoch. Note that the Linear Regressor must be fitted first!

\section{Testing}

Several manual tests were used in the Iris dataset. These tests included all
combinations of flowers with varying features. Some converged and some did not.
I also did some 3 feature learning, to test that functionality. Although there
much more manual tests, two can be found in the example \texttt{prog.py} (commit
tagged prog1). The first is a 2 feature test which takes almost 800 iterations
to converge while the second is a 3 feature test which converges quickly. Both
of these use the first two flower species as they were the easiest to get to
converge.

This needs to be fleshed out with runnable tests for the final portoflio using pytest.

\end{document}

